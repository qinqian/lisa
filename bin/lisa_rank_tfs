""" Lisa interface to rank TFs based on the differential
expression model and whole genome 1kb read count of the samples
"""

from sklearn.cluster import MiniBatchKMeans
from lisa.rank import get_insilico_knockout_tensor_op, rank_by_entropy
from lisa.data import EpigenomeData
from lisa.utils import Weight, one_side_ks_test, convert_name, mannwhitneyu_test, multiple_apply
import h5py

import pandas as pd
import numpy as np
from functools import wraps
import fire
import time
from multiprocessing import Pool, cpu_count
#from pathos.multiprocessing import ProcessingPool as Pool2

def test(args):
    x, y = args
    return mannwhitneyu_test(x,y,how="greater")

class LisaRank(object):
    """rank the TFs given the gene sets
        1. In silico knockout epigenome signal on motif or TF ChIP-seq peak
        2. cluster the selected sample epigenome signal, then compute relative entropy
    """
    def __init__(self, species, prefix, background, foreground, only_newhdf5=False):
        self.epigenome = EpigenomeData(species, None)
        self.species = species
        self.background_genes = np.genfromtxt(background, dtype='str')
        self.foreground_genes = np.genfromtxt(foreground, dtype='str')

        self.diff_gene_num = len(self.foreground_genes)
        self.all_genes = np.concatenate([self.foreground_genes, self.background_genes])
        self.prefix = prefix
        self.only_newhdf5 = only_newhdf5

    def get_hdf(self, dtype):
        """ get corresponding TF binding data type for 100bp window hit
        """
        tfbs_dict = dict(
            motif99=self.epigenome.config.get_motif_index(99),
            motif98=self.epigenome.config.get_motif_index(98),
            motif97=self.epigenome.config.get_motif_index(97),
            chipseq=self.epigenome.config.tf_chipseq
        )
        return tfbs_dict[dtype]

    def direct(self):
        """ rank TF based on TF ChIP-seq peak regulatory potential difference
        between foreground genes and background genes
        """
        # gene x tf sample
        beta_df = self.epigenome.get_beta(self.all_genes)
        # pvalue = beta_df.apply(lambda x: one_side_ks_test(x[:self.diff_gene_num],
        #                                                   x[self.diff_gene_num:]),
        #                        axis=0)

        #pvalue = beta_df.apply(lambda x: mannwhitneyu_test(x[:self.diff_gene_num],
        #                                                   x[self.diff_gene_num:],
        #                                                   how="greater"),
        #                       axis=0)


        pvalue = multiple_apply(test, beta_df, 
                                np.arange(self.diff_gene_num), np.arange(self.diff_gene_num, beta_df.shape[0]), 5)

        pvalue.index = self._get_tfbs_annotation(pvalue.index, 'chipseq')
        pvalue = pvalue.iloc[:, 0]
        pvalue = pvalue.sort_values(ascending=True, inplace=False)
        pvalue.to_csv('%s.lisa_direct.csv' % self.prefix)

    def _prepare_data(self, bin_length):
        # load gene TSS bins
        gene_tss_bin = self.epigenome.get_gene_tss_bin
        gene_bins, gene_bin_centers, gene_chrs, gene_tss = [], [], [], []
        for gene in self.all_genes:
            # center cooridinates, 0-based bin index
            gene_bins.append(int(gene_tss_bin[gene][1]))
            gene_bin_centers.append(int(gene_tss_bin[gene][0]))
            gene_chrs.append(gene.split(":")[0])
            gene_tss.append(int(gene.split(":")[1]))
        gene_bins = np.array(gene_bins)
        gene_bin_centers = np.array(gene_bin_centers)
        gene_tss = np.array(gene_tss)
        bin_center_2d, bin_2d = [], []
        # get the bin center matrix
        for i in range(-100, 100):
            # bin center base coordinate
            bin_center_2d.append(gene_bin_centers + i * bin_length)
            # TSS bin index
            bin_2d.append(gene_bins + i)
        # setup bins for delta rp
        # genes x 200bin
        bin_center_2d = np.vstack(bin_center_2d).T
        bin_2d = np.vstack(bin_2d).T
        dist = (bin_center_2d.T - gene_tss).T

        # compute boundary masked bins
        chrom_bin_mask = self.epigenome.get_chr_boundary_mask(gene_chrs)
        bin_2d_left = bin_2d >= 0 # left chromosome margin
        bin_2d_right = (bin_2d.T - chrom_bin_mask).T <= 0 # right chromosome margin

        boundary_mask = bin_2d_left & bin_2d_right
        bin_2d = bin_2d * boundary_mask

        # compute the weight, each bin center distance from TSS
        weight_obj = Weight(bin_length)  # 1kb windows
        # gene x 200bin
        weight_obj.balance_weight(dist)
        weight = weight_obj.get_weight() * boundary_mask
        return bin_2d, weight

    def knockout(self, epigenome, coefficient, covariates, dtype, new_h5_rp, new_h5_count):
        """ rank TFs by In silico knockout of the epigenome signals

        there are two modes to rank
        species: hg38 or mm10
        prefix: output prefix of Lisa
        epigenome: epigenome type, e.g. H3K27ac
        covariates: True or False, to use GC covariates or not
        coefficient: coefficients csv file

        dtype: choose from 'motif97', 'motif98', 'motif99' or 'chipseq'
               different TF binding data to knock out
        """
        start = time.time()

        self.epigenome.epigenome = epigenome
        # load raw regulatory potential
        raw_reg = self.epigenome.get_RP
        # raw_reg.index = raw_reg.index.map(lambda x:x.upper())  # fix the symbol to uppercase, not necessary, comment

        if new_h5_rp != None: # add hdf5 from fastqs or bigwigs for raw rp
            with h5py.File(new_h5_rp, mode='r') as st:
                eids = list(map(lambda x: x.decode('utf-8'), st["IDs"][...]))
                df = pd.DataFrame(st["RP"][...], index=raw_reg.index,
                                  columns=eids)

                if not self.only_newhdf5:
                    raw_reg = pd.concat([raw_reg, df], axis=1)
                else:
                    raw_reg = df

        if covariates:
            raw_reg = pd.concat([raw_reg, self.epigenome.get_covariates_reg], axis=1)

        # load model coefficients with sample ids in one csv file
        coef = pd.read_csv(coefficient, encoding="ISO-8859-1", index_col=0)
        coef.index = coef.index.astype(str)

        # select foreground and background selected genes
        # and reorder the sample names as the coefficient order
        raw_reg = raw_reg.loc[self.all_genes, list(coef.index)]
        print(raw_reg.shape)

        # lisa prediction
        normalized_reg = np.log2(raw_reg.values+1)
        original_median = np.mean(normalized_reg, axis=0)
        normalized_reg = normalized_reg - original_median
        lisa_prediction = np.dot(normalized_reg, coef.iloc[:, 0].values)
        print(time.time()-start)

        # load chromatin boundary mask array
        bin_length = 1000
        print(bin_length)
        # get refseq information array
        # bin_2d (genes x 200bin, already masked the bins out of chrom end)
        # weight (genes x 200bin, already masked the bins out of chrom end)
        bin_2d, weight = \
            self._prepare_data(bin_length)
        print(bin_2d.shape)
        print(weight.shape)
        print(time.time()-start)

        print('loading read count')
        # add sorted selected_bins list for faster IO loading
        selected_bins = np.unique(bin_2d.reshape(-1))  ## only preserve the bins around gene tss with > 0 read count
        print(selected_bins[:10])

        # load 1kb average read count
        read_count = self.epigenome.get_count(list(coef.index), covariates, new_h5_count, selected_bins=selected_bins[1:])  ## TODO: check only_newh5 option
        print(time.time()-start)
        print('read count loaded')

        # extract signals in bins for all samples
        # gene x bin x sample
        signal = read_count[bin_2d]
        # sample x gene x bin
        signal = signal.transpose(2, 0, 1)
        signal = signal * bin_length # average signal x window size
        # precompute sample x gene x bin
        precompute = signal * weight
        # sample x (gene1_bin1, gene1_bin2...gene2_bin1, gene2_bin2...)
        precompute = \
            precompute.reshape((precompute.shape[0], -1))

        theano_deltarp_function = get_insilico_knockout_tensor_op(lisa_prediction, precompute, coef, original_median)
        # load 100bp to 1kb mapping files
        bin_100_to_1kb = np.load(self.epigenome.config.genome_window_map) # 0-based

        # get TF binding sites in 100bp windows,
        # motif index is 0-based in 100bp windows
        # TF ChIP-seq peak index is 1-based in 100bp windows
        offset = -1 if dtype == 'chipseq' else 0
        delta_rps = []
        sids = []

        # IO do not use function calls
        with h5py.File(self.get_hdf(dtype), mode='r', swmr=True) as store:  ## add swmr > 2.5.0, h5py.version.hdf5_version_tuple > 1.10
        #with h5py.File(self.get_hdf(dtype), mode='r') as store:  ## add swmr > 2.5.0, h5py.version.hdf5_version_tuple > 1.10
            IDs = store['IDs'][...]
            for sid in IDs:
                sids.append(convert_name(sid))

            def get_ISDRP(sid):
                tfbs_index = store[sid][...]  # 100 bp window index vector
                tfbs_binary = np.zeros(bin_100_to_1kb[-1] + 1, dtype=np.uint8)
                tfbs_binary[bin_100_to_1kb[tfbs_index+offset]] = 1
                # gene x bin
                E = tfbs_binary[bin_2d]
                E = E.reshape(1, -1) # 1 x (gene, bin)
                E = np.logical_not(E)
                return theano_deltarp_function(E)

            ## using 4 threads
            # any original python Pool neds the function to be pickled
            # not work for nested functions
            # with Pool(4) as pool: 
            #     delta_rps = pool.map(get_ISDRP, IDs) 

            # try pathos dill to serialize python function
            # with Pool2(4) as pool: 
            #     delta_rps = pool.map(get_ISDRP, IDs) 

            for sid in IDs:
                 delta_rps.append(get_ISDRP(sid))
            #    tfbs_index = store[sid][...]
            #    tfbs_binary = np.zeros(bin_100_to_1kb[-1] + 1, dtype=np.int32)
            #    # map 100bp windows back to 1kb windows
            #    # TODO: add DNase-seq peak filter
            #    tfbs_binary[bin_100_to_1kb[tfbs_index+offset]] = 1
            #    # gene x bin
            #    E = tfbs_binary[bin_2d]
            #    E = E.reshape(1, -1) # 1 x (gene, bin)
            #    E = np.logical_not(E)
            #    delta_rps.append(theano_deltarp_function(E))
            #    sids.append(convert_name(sid))

        delta_rps = np.vstack(delta_rps).T # gene x tfbs (motif or peak)
        delta_rp_df = pd.DataFrame(delta_rps, columns=sids, index=self.all_genes)
        print('statistics done')
        print(time.time()-start)
        delta_rp_df.columns = self._get_tfbs_annotation(delta_rp_df.columns, dtype)
        # pvalue = delta_rp_df.apply(lambda x: one_side_ks_test(x[:self.diff_gene_num],
        #                                                       x[self.diff_gene_num:]),
        #                            axis=0)
        #pvalue = delta_rp_df.apply(lambda x: mannwhitneyu_test(x[:self.diff_gene_num],
        #                                                       x[self.diff_gene_num:],
        #                                                       how="greater"),
        #                           axis=0)
        # 4 processes to apply statistical test across ISD-RP
        pvalue = multiple_apply(test, delta_rp_df, 
                                np.arange(self.diff_gene_num), np.arange(self.diff_gene_num, delta_rp_df.shape[0]), 4)
        pvalue = pvalue.iloc[:, 0]

        delta_rp_df.loc['pvalue', :] = pvalue
        delta_rp_df = delta_rp_df.iloc[:, np.argsort(pvalue)]

        diff_binary = np.zeros(len(self.all_genes)+1) # add 1 to align the p-value
        diff_binary[: self.diff_gene_num] = 1

        delta_rp_df.loc[:, 'diff_status'] = diff_binary
        delta_rp_df.to_csv("%s.%s.%s.csv" % (self.prefix, self.epigenome.epigenome, dtype))
        pvalue = pvalue.sort_values(inplace=False)
        pvalue.to_csv("%s.%s.%s.p_value.csv" % (self.prefix, self.epigenome.epigenome, dtype))

    def _get_tfbs_annotation(self, cols, dtype):
        # annotation of chipseq dc ids or motif ids
        cols_new = []
        for i in cols:
            try:
                cols_new.append(i.decode('utf-8').split('_')[0])
            except:
                cols_new.append(i.split('_')[0])
        if dtype == 'chipseq':
            try:
                selection_ids = list(map(int, cols_new))
            except ValueError: ## imputation data without cistrome id to search
                return cols
            meta = pd.read_table(self.epigenome.config.tf_chipseq_meta, 
                                 encoding="ISO-8859-1",
                                 index_col=0)
            #meta = pd.read_table(self.epigenome.config.get_basic_meta,
            #                     encoding="ISO-8859-1",
            #                     index_col=0)
            #selection = 'FactorName'
            selection = 'factor'
        else:
            selection_ids = cols_new
            selection = 'symbol'
            meta = pd.read_table(self.epigenome.config.get_motif_meta, encoding="ISO-8859-1", index_col=0)
        annotation = meta.loc[selection_ids, selection]
        result = list(map(lambda x: '|'.join(map(str, x)), 
                          zip(annotation.index, 
                              annotation)))
        return result

    def entropy(self, epigenome, coefficient, covariates, dtype, new_h5):
        """ first cluster selected samples by regions
        then evaluate kl divergence
        """
        self.epigenome.epigenome = epigenome

        # load model coefficients with sample ids in one csv file
        coef = pd.read_csv(coefficient, encoding="ISO-8859-1", index_col=0)
        coef.index = coef.index.astype(str)

        # load 1kb average read count

        read_count = self.epigenome.get_count(list(coef.index), covariates, new_h5, only_newh5=self.only_newhdf5)

        # load chromatin boundary mask array
        bin_length = 1000
        # get refseq information array
        # bin_2d (genes x 200bin, already masked the bins out of chrom end)
        # weight (genes x 200bin, already masked the bins out of chrom end)
        bin_2d, _ = \
            self._prepare_data(bin_length)

        bin_1d = bin_2d.reshape(-1)
        bin_1d = np.unique(bin_1d, return_index=False)        # remove duplicates of bins
        bin_1d = bin_1d[1:]                                   # the first element is the masked bin
        read_count = read_count[bin_1d]

        norm = np.log2(read_count+1)
        norm = norm - np.mean(norm)
        norm = norm * np.abs(coef.iloc[:, 0].values)

        n_clusters = 10  # 2,5,10 cluster?
        kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=0).fit(norm)
        labels_order = np.argsort(kmeans.labels_)
        norm = norm[labels_order]
        labels = kmeans.labels_[labels_order]
        cluster_stat = np.unique(labels, return_counts=True)

        prop_dict = {}
        cluster_dict = {}
        for cluster, ct in zip(*cluster_stat):
            cluster_dict[cluster] = ct

        # load 100bp to 1kb mapping files
        bin_100_to_1kb = np.load(self.epigenome.config.genome_window_map) # 0-based
        offset = -1 if dtype == 'chipseq' else 0
        with h5py.File(self.get_hdf(dtype), mode='r') as store:
            ids = store['IDs']
            for tfbs_id in ids:
                # TODO: add DNase-seq peak filter
                tfbs_index = store[tfbs_id][...] + offset
                # 1kb window
                tfbs_bin = np.zeros(bin_100_to_1kb[-1] + 1, dtype=np.int32)
                # 1kb 0-1 vector
                tfbs_bin[bin_100_to_1kb[tfbs_index]] = 1
                is_s = np.asarray(tfbs_bin, dtype=np.bool)
                is_s = is_s[bin_1d]           # the window near the genes
                is_s = is_s[labels_order]     # order by cluster label
                labels_tfbs = labels[is_s]    # get the labels
                stat = {}
                # cluster label distribution for the tfbs data
                for i, j in zip(*np.unique(labels_tfbs, return_counts=True)):
                    stat[i] = j
                prop_dict[tfbs_id] = []
                for key in cluster_dict:
                    prop_dict[tfbs_id].append(stat.get(key,0))

        prop_dict['cluster_number'] = []
        for key in cluster_dict:
            prop_dict['cluster_number'].append(cluster_dict[key])
        prop =  pd.DataFrame(prop_dict, index=['cluster%s' %s for s in cluster_dict])
        prop.to_csv('%s.%s.%s.cluster_stat.csv' % (self.prefix, self.epigenome.epigenome, dtype))

        divergence = rank_by_entropy(prop)
        divergence.index = self._get_tfbs_annotation(divergence.index, dtype)
        divergence.to_csv('%s.%s.%s.entropy_rank.csv' % (self.prefix, self.epigenome.epigenome, dtype))

if __name__ == '__main__':
    fire.Fire(LisaRank)
